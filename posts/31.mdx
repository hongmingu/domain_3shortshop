---
title: typescript 머신러닝 기초2
description: "typescript 로 머신러닝 기본 프로그램 만들기2"
date: July 31 2023
---
지난 시간에 이어 텐서플로우를 이용해보는 실습이다.

### NLP 자연어 처리 해보기


이번엔 간단한 문자열을 입력해서 다른 문자열을 출력해볼 것이다.
챗봇이나 기계번역등에 사용된다.



```
npm install @tensorflow/tfjs @types/node
```

텐서플로우를 설치해주고



#### NLP 텐서플로 코드

```
import * as tf from '@tensorflow/tfjs';

// 모델의 파라미터 정의
const inputVocabSize = 100;  // 입력 어휘 사전의 크기
const targetVocabSize = 100;  // 목표 어휘 사전의 크기
const inputEmbeddingSize = 256;  // 입력 임베딩 벡터의 차원
const targetEmbeddingSize = 256;  // 목표 임베딩 벡터의 차원
const transformerLayers = 2;  // Transformer 레이어의 수

// Transformer 모델 생성
const model = tf.sequential();
model.add(tf.layers.embedding({
    inputDim: inputVocabSize,
    outputDim: inputEmbeddingSize,
    inputLength: 10,  // 입력 시퀀스의 길이
}));
for (let i = 0; i < transformerLayers; ++i) {
    model.add(tf.layers.transformer({
        numLayers: 2,
        units: 256,
        numHeads: 2,
        ffDim: 128,
    }));
}
model.add(tf.layers.dense({units: targetVocabSize, activation: 'softmax'}));

// 모델 컴파일
model.compile({optimizer: 'adam', loss: 'sparseCategoricalCrossentropy'});

// 임의의 학습 데이터 생성
const xTrain = tf.randomUniform([100, 10], 0, inputVocabSize, 'int32');  // 100개의 입력 시퀀스
const yTrain = tf.randomUniform([100, 10], 0, targetVocabSize, 'int32');  // 100개의 목표 시퀀스

// 모델 학습
await model.fit(xTrain, yTrain, {epochs: 50});

// 모델을 사용한 예측
const xPredict = tf.randomUniform([1, 10], 0, inputVocabSize, 'int32');  // 1개의 입력 시퀀스
const prediction = model.predict(xPredict) as tf.Tensor;

// 예측 결과 출력
prediction.print();


```

이 예제에서는 입력 어휘 사전과 목표 어휘 사전의 크기, 임베딩 벡터의 차원, Transformer 레이어의 수 등을 파라미터로 가지는 Transformer 모델을 생성한다. 

이 모델은 입력 시퀀스를 받아 목표 시퀀스를 출력하는 역할을 한다.

그런 다음 임의의 입력 시퀀스와 목표 시퀀스를 생성하여 모델을 학습시키고, 새로운 입력 시퀀스에 대한 예측을 수행한다.

여기서 모델의 파라미터를 튜닝해보자

#### 하이퍼파라미터 튜닝

모델의 성능을 향상시키는 방법 중 하나는 하이퍼파라미터 튜닝이다. 
하이퍼파라미터는 학습 프로세스에 영향을 미치지만 데이터에서 직접 학습되지 않는 모델의 구성 요소를 의미한다. 

예를 들어 학습률(learning rate), 배치 크기(batch size), 에폭 수(epochs), 은닉층의 수나 유닛의 수 등이 있다.

하이퍼파라미터 튜닝의 기본적인 아이디어는 다양한 하이퍼파라미터 값에 대해 모델을 학습시키고, 검증 세트에서 가장 좋은 성능을 보이는 하이퍼파라미터를 선택하는 것이다.

```
import * as tf from '@tensorflow/tfjs';

// 모델 생성
const model = tf.sequential();
model.add(tf.layers.dense({units: 1, inputShape: [1]}));

// 임의의 학습 데이터 생성
const xTrain = tf.randomNormal([100, 1]);
const yTrain = tf.randomNormal([100, 1]);

// 검증 데이터 생성
const xVal = tf.randomNormal([20, 1]);
const yVal = tf.randomNormal([20, 1]);

// 튜닝할 학습률 정의
const learningRates = [0.01, 0.001, 0.0001];

let bestValLoss = Number.POSITIVE_INFINITY;
let bestLearningRate: number;

for (const learningRate of learningRates) {
    // 모델 컴파일
    model.compile({optimizer: tf.train.sgd(learningRate), loss: 'meanSquaredError'});

    // 모델 학습
    const history = await model.fit(xTrain, yTrain, {
        epochs: 10,
        validationData: [xVal, yVal],
        verbose: 0
    });

    // 검증 손실 확인
    const valLoss = history.history.loss[history.history.loss.length - 1] as number;
    if (valLoss < bestValLoss) {
        bestValLoss = valLoss;
        bestLearningRate = learningRate;
    }
}

console.log(`Best learning rate: ${bestLearningRate}`);

```

이 코드에서는 세 가지 다른 학습률에 대해 모델을 학습시키고 검증 데이터에 대한 손실을 계산한다. 
이 때 가장 낮은 검증 손실을 보이는 학습률이 최적의 학습률로 선택된다.



### 그리드 탐색

그리드 탐색은 가장 간단하면서도 가장 많이 사용되는 방법 중 하나다. 
이 방법은 하이퍼파라미터의 가능한 모든 조합을 탐색하므로 시간이 많이 소요될 수 있긴 하다. 
그러나 모든 조합을 평가하기 때문에 가장 좋은 하이퍼파라미터 조합을 찾을 가능성이 높다.

#### 코드 예시

```
import * as tf from '@tensorflow/tfjs';

// 모델 생성
const model = tf.sequential();
model.add(tf.layers.dense({units: 1, inputShape: [1]}));

// 임의의 학습 데이터 생성
const xTrain = tf.randomNormal([100, 1]);
const yTrain = tf.randomNormal([100, 1]);

// 검증 데이터 생성
const xVal = tf.randomNormal([20, 1]);
const yVal = tf.randomNormal([20, 1]);

// 튜닝할 하이퍼파라미터 정의
const learningRates = [0.01, 0.001, 0.0001];
const epochs = [10, 20, 30];

let bestValLoss = Number.POSITIVE_INFINITY;
let bestParams: {learningRate: number, epoch: number};

for (const learningRate of learningRates) {
    for (const epoch of epochs) {
        // 모델 컴파일
        model.compile({optimizer: tf.train.sgd(learningRate), loss: 'meanSquaredError'});

        // 모델 학습
        const history = await model.fit(xTrain, yTrain, {
            epochs: epoch,
            validationData: [xVal, yVal],
            verbose: 0
        });

        // 검증 손실 확인
        const valLoss = history.history.loss[history.history.loss.length - 1] as number;
        if (valLoss < bestValLoss) {
            bestValLoss = valLoss;
            bestParams = {learningRate, epoch};
        }
    }
}

console.log(`Best params: ${JSON.stringify(bestParams)}`);

```
위의 코드는 각 학습률과 에폭 수 조합에 대해 모델을 학습하고 검증 데이터에 대한 손실을 알아낸다.
 가장 낮은 검증 손실을 보이는 학습률과 에폭 수의 조합이 최적의 조합으로 결정된다.

그리드 탐색은 브루트포스 방식으로 모든 가능한 하이퍼파라미터의 조합을 탐색하므로 계산 비용이 많이 들 수 있다.
 따라서 하이퍼파라미터의 수가 많거나 검증 과정이 시간이 오래 걸리는 경우에는 랜덤 탐색이나 베이지안 최적화와 같은 다른 방법을 고려해 볼 수 있다.


### 랜덤 탐색


```
import * as tf from '@tensorflow/tfjs';

// 모델 생성
const model = tf.sequential();
model.add(tf.layers.dense({units: 1, inputShape: [1]}));

// 임의의 학습 데이터 생성
const xTrain = tf.randomNormal([100, 1]);
const yTrain = tf.randomNormal([100, 1]);

// 검증 데이터 생성
const xVal = tf.randomNormal([20, 1]);
const yVal = tf.randomNormal([20, 1]);

// 튜닝할 하이퍼파라미터 범위 정의
const minLearningRate = 0.0001;
const maxLearningRate = 0.1;
const minEpochs = 5;
const maxEpochs = 50;

let bestValLoss = Number.POSITIVE_INFINITY;
let bestParams: {learningRate: number, epoch: number};

// 랜덤 탐색 수행
for (let i = 0; i < 100; i++) {
    // 하이퍼파라미터 랜덤하게 선택
    const learningRate = Math.random() * (maxLearningRate - minLearningRate) + minLearningRate;
    const epochs = Math.floor(Math.random() * (maxEpochs - minEpochs + 1)) + minEpochs;

    // 모델 컴파일
    model.compile({optimizer: tf.train.sgd(learningRate), loss: 'meanSquaredError'});

    // 모델 학습
    const history = await model.fit(xTrain, yTrain, {
        epochs: epochs,
        validationData: [xVal, yVal],
        verbose: 0
    });

    // 검증 손실 확인
    const valLoss = history.history.loss[history.history.loss.length - 1] as number;
    if (valLoss < bestValLoss) {
        bestValLoss = valLoss;
        bestParams = {learningRate, epoch: epochs};
    }
}

console.log(`Best params: ${JSON.stringify(bestParams)}`);


```
이 코드는 학습률과 에폭 수를 랜덤하게 선택하여 100번의 학습을 수행하고 가장 낮은 검증 손실을 가진 학습률과 에폭 수의 조합을 알아낸다. 

랜덤 탐색은 하이퍼파라미터 공간이 매우 크거나 그리드 탐색으로 모든 조합을 탐색하는 것이 비현실적일 때 유용하다.


### 베이지안 최적화

여기는 node-gp라는 패키지를 이용한다

```
npm install node-gp

```

패키지를 설치했으면 최적화를 수행할 수 있다.

```
import * as tf from '@tensorflow/tfjs';
import { minimize } from 'node-gp';

// 모델 생성
const model = tf.sequential();
model.add(tf.layers.dense({units: 1, inputShape: [1]}));

// 임의의 학습 데이터 생성
const xTrain = tf.randomNormal([100, 1]);
const yTrain = tf.randomNormal([100, 1]);

// 검증 데이터 생성
const xVal = tf.randomNormal([20, 1]);
const yVal = tf.randomNormal([20, 1]);

// 학습 함수 생성
const train = async (learningRate: number, epochs: number) => {
    // 모델 컴파일
    model.compile({optimizer: tf.train.sgd(learningRate), loss: 'meanSquaredError'});

    // 모델 학습
    const history = await model.fit(xTrain, yTrain, {
        epochs: epochs,
        validationData: [xVal, yVal],
        verbose: 0
    });

    // 검증 손실 반환
    return history.history.loss[history.history.loss.length - 1] as number;
};

// 베이지안 최적화 수행
const bestParams = minimize(train, [{name: 'learningRate', min: 0.0001, max: 0.1}, {name: 'epochs', min: 5, max: 50}]);

console.log(`Best params: ${JSON.stringify(bestParams)}`);

```

이 코드는 minimize 함수를 사용하여 train 함수의 반환값을 최소화하는 하이퍼파라미터를 찾는다.

 minimize 함수는 첫 번째 인자로 최적화할 함수를 두 번째 인자로 최적화할 하이퍼파라미터의 범위를 받는다.

train함수의 반환값은 검증 손실을 의미한다.

### 그래디언트 최적화

그래디언트 기반 최적화는 일반적으로 신경망의 가중치를 최적화하는 데 사용되지만 학습 가능한 하이퍼파라미터에도 적용할 수 있다. 
그러나 TensorFlow.js는 현재 학습률 같은 하이퍼파라미터에 대한 그래디언트를 자동으로 계산하는 기능을 지원하지 않는다. 
이를 수행하기 위해서는 각각의 하이퍼파라미터에 대한 그래디언트를 수동으로 계산해야 하며 이는 매우 복잡하고 에러가 발생하기 쉽다.

그러므로 대신 TensorFlow.js의 Optimizer.minimize 메소드를 사용하여 가중치를 그래디언트 기반 방법으로 최적화할 수 있다. 
이 메소드는 모델의 손실 함수의 그래디언트를 계산하고 이를 이용하여 모델의 가중치를 업데이트한다.


```
import * as tf from '@tensorflow/tfjs';

// 모델 생성
const model = tf.sequential();
model.add(tf.layers.dense({units: 1, inputShape: [1]}));

// 임의의 학습 데이터 생성
const xs = tf.tensor2d([1, 2, 3, 4], [4, 1]);
const ys = tf.tensor2d([1, 3, 5, 7], [4, 1]);

// 손실 함수 정의
const loss = (pred: tf.Tensor, label: tf.Tensor) => pred.sub(label).square().mean();

// 그래디언트 기반 최적화
const learningRate = 0.01;
const optimizer = tf.train.sgd(learningRate);

for (let i = 0; i < 100; i++) {
    optimizer.minimize(() => loss(model.predict(xs) as tf.Tensor, ys));
}

```

이 코드는 모델의 예측과 실제 레이블 간의 평균 제곱 오차를 최소화하기 위해 SGD 최적화기를 사용한다. 
optimizer.minimize 메소드는 제공된 손실 함수의 그래디언트를 계산하고 이를 이용하여 모델의 가중치를 업데이트한다.

### 진화 알고리즘

진화 알고리즘은 생물학적 진화 과정을 모방하여 하이퍼파라미터를 최적화한다. 개체 즉 하이퍼파라미터 조합은 피트니스에 따라 선택되고 변형되며 시간이 지남에 따라 더 좋은 하이퍼파라미터 조합이 진화하는 방식으로 동작한다.

진화 알고리즘은 TensorFlow.js에 내장되어 있지 않기 때문에 이를 수행하기 위해서는 추가적인 라이브러리가 필요하다. 

여기에서는 간단한 진화 알고리즘 구현을 위해 geneticalgorithm 패키지를 사용할 것이다.

```
npm install geneticalgorithm

```


이제 설치가 끝났으면

다음과 같이 작성하자

```
import * as tf from '@tensorflow/tfjs';
import * as ga from 'geneticalgorithm';

// 모델 생성
const model = tf.sequential();
model.add(tf.layers.dense({units: 1, inputShape: [1]}));

// 임의의 학습 데이터 생성
const xTrain = tf.randomNormal([100, 1]);
const yTrain = tf.randomNormal([100, 1]);

// 검증 데이터 생성
const xVal = tf.randomNormal([20, 1]);
const yVal = tf.randomNormal([20, 1]);

// 학습 함수 생성
const train = async (learningRate: number, epochs: number) => {
    // 모델 컴파일
    model.compile({optimizer: tf.train.sgd(learningRate), loss: 'meanSquaredError'});

    // 모델 학습
    const history = await model.fit(xTrain, yTrain, {
        epochs: epochs,
        validationData: [xVal, yVal],
        verbose: 0
    });

    // 검증 손실 반환
    return history.history.loss[history.history.loss.length - 1] as number;
};

// 피트니스 함수 생성
const fitnessFunction = (candidate: any) => {
    return train(candidate.learningRate, candidate.epochs);
};

// 초기 인구 생성
const population = Array.from({ length: 100 }, () => ({
    learningRate: Math.random() * 0.1, 
    epochs: Math.floor(Math.random() * 50)
}));

// 진화 알고리즘 설정
const config = {
    mutationFunction: (candidate: any) => ({
        learningRate: candidate.learningRate + Math.random() * 0.01 - 0.005,
        epochs: candidate.epochs + Math.floor(Math.random() * 10 - 5)
    }),
    crossoverFunction: (mother: any, father: any) => ({
        learningRate: (mother.learningRate + father.learningRate) / 2,
        epochs: Math.floor((mother.epochs + father.epochs) / 2)
    }),
    fitnessFunction: fitnessFunction,
    population: population,
    populationSize: 100
};

// 진화 알고리즘 실행
const geneticAlgorithm = ga(config);
const bestCandidate = geneticAlgorithm.evolve();

console.log(`Best params: ${JSON.stringify(bestCandidate)}`);

```

이 코드는 각 하이퍼파라미터 조합이 검증 손실에 따라 선택되고 
돌연변이와 교차를 통해 새로운 후보를 생성하는 진화 알고리즘을 사용하여 하이퍼파라미터를 최적화한다. 
geneticalgorithm 패키지는 각 후보의 피트니스를 계산하고 가장 피트니스가 높은 후보를 선택하는 등의 작업을 자동으로 수행한다.