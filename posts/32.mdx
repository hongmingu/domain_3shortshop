---
title: Swift 음성 데이터 머신러닝 기초 실습
description: "Swift 로 음성 데이터 머신러닝 환경설정 및 기본 프로그램 만들기"
date: July 31 2023
---
음성 데이터와 머신러닝의 조합은 실생활에서 많이 사용된다.
이를 이용하면 음성 인식, 음성 합성, 감정 분석, 음성 변환 등 다양한 응용이 가능하다.

Apple은 iOS 개발자들이 이러한 응용을 더 쉽게 구현할 수 있도록 Core ML과 같은 머신러닝 프레임워크를 제공하고 있다.
 Core ML을 사용하면 훈련된 머신러닝 모델을 iOS 앱에 쉽게 통합할 수 있다. 
 또한 Apple은 자체적으로 훈련된 몇 가지 모델을 제공하고 있는데 이를 사용하면 간단한 작업에 대해 모델을 직접 훈련할 필요가 없다.

음성 인식에 대한 간단한 예제를 보자. 이 예제에서는 Apple의 Speech 프레임워크를 사용하여 iOS 기기의 마이크로부터 오는 음성을 실시간으로 텍스트로 변환한다.

### 기초 예제

```
import UIKit
import Speech

class ViewController: UIViewController, SFSpeechRecognizerDelegate {
    
    @IBOutlet weak var textView: UITextView!
    @IBOutlet weak var microphoneButton: UIButton!
    
    private let speechRecognizer = SFSpeechRecognizer(locale: Locale(identifier: "en-US"))!
    private var recognitionRequest: SFSpeechAudioBufferRecognitionRequest?
    private var recognitionTask: SFSpeechRecognitionTask?
    private let audioEngine = AVAudioEngine()
    
    override func viewDidLoad() {
        super.viewDidLoad()
        
        microphoneButton.isEnabled = false
        speechRecognizer.delegate = self
        
        SFSpeechRecognizer.requestAuthorization { (authStatus) in
            var isButtonEnabled = false
            switch authStatus {
            case .authorized:
                isButtonEnabled = true
            case .denied:
                isButtonEnabled = false
            case .restricted:
                isButtonEnabled = false
            case .notDetermined:
                isButtonEnabled = false
            }
            OperationQueue.main.addOperation() {
                self.microphoneButton.isEnabled = isButtonEnabled
            }
        }
    }
    
    @IBAction func microphoneTapped(_ sender: AnyObject) {
        if audioEngine.isRunning {
            audioEngine.stop()
            recognitionRequest?.endAudio()
            microphoneButton.isEnabled = false
            microphoneButton.setTitle("Start Recording", for: .normal)
        } else {
            startRecording()
            microphoneButton.setTitle("Stop Recording", for: .normal)
        }
    }
    
    func startRecording() {
        
        // Cancel the previous task if it's running.
        recognitionTask?.cancel()
        self.recognitionTask = nil
        
        let audioSession = AVAudioSession.sharedInstance()
        try! audioSession.setCategory(AVAudioSession.Category.record, mode: .measurement, options: .duckOthers)
        try! audioSession.setActive(true, options: .notifyOthersOnDeactivation)
        
        recognitionRequest = SFSpeechAudioBufferRecognitionRequest()
        
        let inputNode = audioEngine.inputNode
        
        guard let recognitionRequest = recognitionRequest else {
            fatalError("Unable to create a SFSpeechAudioBufferRecognitionRequest object")
        }
        
        recognitionRequest.shouldReportPartialResults = true
        
        recognitionTask = speechRecognizer.recognitionTask(with: recognitionRequest, resultHandler: { (result, error) in
            
            var isFinal = false
            
            if result != nil {
                self.textView.text = result?.bestTranscription.formattedString
                isFinal = (result?.isFinal)!
            }
            
            if error != nil || isFinal {
                self.audioEngine.stop()
                inputNode.removeTap(onBus: 0)
                
                self.recognitionRequest = nil
                self.recognitionTask = nil
                
                self.microphoneButton.isEnabled = true
            }
        })
        
        let recordingFormat = inputNode.outputFormat(forBus: 0)
        inputNode.installTap(onBus: 0, bufferSize: 1024, format: recordingFormat) { (buffer, when) in
            self.recognitionRequest?.append(buffer)
        }
        
        audioEngine.prepare()
        try! audioEngine.start()
    }
    
    func speechRecognizer(_ speechRecognizer: SFSpeechRecognizer, availabilityDidChange available: Bool) {
        if available {
            microphoneButton.isEnabled = true
        } else {
            microphoneButton.isEnabled = false
        }
    }
}

```

위 코드는 사용자의 음성을 실시간으로 텍스트로 변환하여 화면에 표시한다. 
또한 사용자가 "Stop Recording" 버튼을 누르면 음성 인식을 중지한다.


### 실전 응용

#### 음성 명령 기반 UI

사용자의 음성 명령을 텍스트로 변환한 후 이 텍스트를 분석하여 사용자의 의도를 이해하고 앱의 기능을 제어해보자.
 예를 들어 "음악을 재생하십시오", "다음 곡으로 이동하십시오" 등의 명령을 받아 음악 재생 앱을 제어하는 게 가능하다.

이 앱은 사용자의 음성 명령을 텍스트로 변환하고 "재생", "정지", "다음", "이전" 이라는 명령어를 인식하여 음악 플레이어의 기능을 제어한다.

```
import SwiftUI
import Combine
import Speech

struct ContentView: View {
    @State private var isRecording = false
    @State private var lastCommand: String = ""
    
    var body: some View {
        VStack {
            Text("Last command: \(lastCommand)")
            Button(action: {
                self.isRecording.toggle()
                if self.isRecording {
                    startRecording()
                } else {
                    stopRecording()
                }
            }) {
                Text("\(isRecording ? "Stop" : "Start") recording")
            }
        }
    }
    
    private let speechRecognizer = SFSpeechRecognizer()
    private let audioEngine = AVAudioEngine()
    private var recognitionRequest: SFSpeechAudioBufferRecognitionRequest?
    private var recognitionTask: SFSpeechRecognitionTask?
    
    private func startRecording() {
        recognitionTask?.cancel()
        recognitionTask = nil
        recognitionRequest = SFSpeechAudioBufferRecognitionRequest()
        
        guard let recognitionRequest = recognitionRequest else { return }
        
        let inputNode = audioEngine.inputNode
        let recordingFormat = inputNode.outputFormat(forBus: 0)
        inputNode.installTap(onBus: 0, bufferSize: 1024, format: recordingFormat) { buffer, _ in
            recognitionRequest.append(buffer)
        }
        
        audioEngine.prepare()
        try? audioEngine.start()
        
        recognitionTask = speechRecognizer?.recognitionTask(with: recognitionRequest) { result, error in
            if let result = result {
                self.lastCommand = result.bestTranscription.formattedString
                performCommand(result.bestTranscription.formattedString)
            }
        }
    }
    
    private func stopRecording() {
        audioEngine.stop()
        recognitionRequest?.endAudio()
    }
    
    private func performCommand(_ command: String) {
        switch command {
        case "재생":
            // 음악 재생 코드
            break
        case "정지":
            // 음악 정지 코드
            break
        case "다음":
            // 다음 곡 재생 코드
            break
        case "이전":
            // 이전 곡 재생 코드
            break
        default:
            break
        }
    }
}

```

위 코드는 사용자의 음성을 텍스트로 변환하여 lastCommand에 저장하고 performCommand 메서드에서 이 텍스트를 분석하여 앱의 음악 플레이어를 제어한다. 
사용자가 "재생", "정지", "다음", "이전" 이라는 명령어를 말하면 앱은 해당 명령어에 따라 음악 플레이어를 제어하게 된다.

