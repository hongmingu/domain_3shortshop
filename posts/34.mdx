---
title: Swift 음성 데이터 머신러닝 기초 3
description: "Swift 로 음성 데이터 머신러닝 기본 프로그램 만들기 3 언어 학습 앱 제작"
date: July 31 2023
---
지난 게시에 이어 이번엔 언어 학습 도우미 앱을 만들어보자
사용자가 외국어 문장을 말하면 앱이 이를 텍스트로 변환하고 문법이나 발음을 점검해준다. 
언어 학습을 더 효과적으로 돕는 도우미 앱이다.

### 기본 설명
이 앱은 사용자의 음성 입력을 텍스트로 변환하고 그 텍스트가 사전에 정의된 문장과 일치하는지 확인한다. 
이 예제에서는 간단하게 문장의 일치 여부만 확인하지만 실제 상용화 앱에서는 문장의 문법적 정확성 발음의 정확성 등을 확인하고 피드백을 제공할 수 있을 것이다.

#### 예제 코드


```
import SwiftUI
import Combine
import Speech

struct ContentView: View {
    @State private var isRecording = false
    @State private var recognizedText: String = ""
    @State private var targetSentence: String = "Hello, world!"
    @State private var isCorrect: Bool = false
    
    var body: some View {
        VStack {
            Text("Target sentence: \(targetSentence)")
                .font(.title)
                .padding(.top)
            
            Text("Your sentence: \(recognizedText)")
                .font(.title2)
                .padding()
            
            if isCorrect {
                Text("Correct!")
                    .font(.headline)
                    .foregroundColor(.green)
            } else {
                Text("Incorrect")
                    .font(.headline)
                    .foregroundColor(.red)
            }
            
            Button(action: {
                self.isRecording.toggle()
                if self.isRecording {
                    startRecording()
                } else {
                    stopRecording()
                }
            }) {
                Text("\(isRecording ? "Stop" : "Start") recording")
                    .foregroundColor(.white)
                    .padding()
                    .background(isRecording ? Color.red : Color.blue)
                    .cornerRadius(10)
            }
            .padding(.bottom)
        }
    }
    
    // ...
}

```


Speech 프레임워크를 사용하여 사용자의 음성 입력을 텍스트로 변환한다. 
사용자가 녹음 버튼을 누르면 startRecording 함수를 호출하여 녹음을 시작하고 다시 녹음 버튼을 누르면 stopRecording 함수를 호출하여 녹음을 중지한다. 
녹음이 중지되면 인식된 텍스트를 recognizedText에 저장하고 이 텍스트가 목표 문장과 일치하는지 확인하여 isCorrect의 값을 설정하게 된다.

### 녹음시작, 녹음종료 구현하기

개인의 입맛에 따라 구현할 수 있지만 나같은 경우는 다음과 같이 구현했다.

```
import SwiftUI
import Combine
import Speech

struct ContentView: View {
    @State private var isRecording = false
    @State private var recognizedText: String = ""
    @State private var targetSentence: String = "Hello, world!"
    @State private var isCorrect: Bool = false
    @State private var recognitionRequest: SFSpeechAudioBufferRecognitionRequest?
    @State private var recognitionTask: SFSpeechRecognitionTask?
    let audioEngine = AVAudioEngine()
    let speechRecognizer = SFSpeechRecognizer()
    
    var body: some View {
        VStack {
            Text("Target sentence: \(targetSentence)")
                .font(.title)
                .padding(.top)
            
            Text("Your sentence: \(recognizedText)")
                .font(.title2)
                .padding()
            
            if isCorrect {
                Text("Correct!")
                    .font(.headline)
                    .foregroundColor(.green)
            } else {
                Text("Incorrect")
                    .font(.headline)
                    .foregroundColor(.red)
            }
            
            Button(action: {
                self.isRecording.toggle()
                if self.isRecording {
                    do {
                        try self.startRecording()
                    } catch {
                        print("Failed to start recording: \(error)")
                    }
                } else {
                    self.stopRecording()
                }
            }) {
                Text("\(isRecording ? "Stop" : "Start") recording")
                    .foregroundColor(.white)
                    .padding()
                    .background(isRecording ? Color.red : Color.blue)
                    .cornerRadius(10)
            }
            .padding(.bottom)
        }
    }
    
    func startRecording() throws {
        isCorrect = false
        recognizedText = ""
        
        let audioSession = AVAudioSession.sharedInstance()
        try audioSession.setCategory(.record, mode: .measurement, options: .duckOthers)
        try audioSession.setActive(true, options: .notifyOthersOnDeactivation)
        
        recognitionRequest = SFSpeechAudioBufferRecognitionRequest()
        guard let recognitionRequest = recognitionRequest else { return }
        
        let inputNode = audioEngine.inputNode
        recognitionRequest.shouldReportPartialResults = true
        
        recognitionTask = speechRecognizer?.recognitionTask(with: recognitionRequest, resultHandler: { (result, error) in
            if let result = result {
                self.recognizedText = result.bestTranscription.formattedString
                if self.recognizedText == self.targetSentence {
                    self.isCorrect = true
                }
            } else if let error = error {
                print("Recognition failed: \(error)")
            }
        })
        
        let recordingFormat = inputNode.outputFormat(forBus: 0)
        inputNode.installTap(onBus: 0, bufferSize: 1024, format: recordingFormat) { (buffer, _) in
            recognitionRequest.append(buffer)
        }
        
        audioEngine.prepare()
        try audioEngine.start()
    }
    
    func stopRecording() {
        audioEngine.stop()
        audioEngine.inputNode.removeTap(onBus: 0)
        
        recognitionRequest?.endAudio()
        recognitionTask?.cancel()
        
        recognitionRequest = nil
        recognitionTask = nil
    }
}

```

이 예제는 startRecording 함수에서 오디오 세션을 설정하고 음성 인식 작업을 시작한다. 
인식 작업은 recognitionTask 변수에 저장되고 이 작업은 인식 결과가 나오거나 에러가 발생할 때마다 핸들러를 호출하게 된다. 
결과 핸들러에서는 인식된 텍스트를 recognizedText 변수에 저장하고 이 텍스트가 목표 문장과 일치하는지 확인하여 isCorrect의 값을 설정한다.

stopRecording 함수에서는 오디오 엔진을 정지하고 인식 작업을 취소한다. 
이후 오디오 입력 노드에서 탭을 제거하고 인식 요청과 인식 작업을 nil로 설정하여 리소스를 정리한다.

실행하려면 앱의 Info.plist에 NSSpeechRecognitionUsageDescription 키를 추가하여 사용자로부터 음성 인식 권한을 요청해야 하니 주의.

### UI/UX 개선하기

디자인이 얼마나 중요한지 누구나 알지 않을까?

SwiftUI의 ZStack, VStack, 그리고 .overlay()와 같은 기능을 활용하여 UI를 개선해보자.


```
import SwiftUI
import Combine
import Speech

struct ContentView: View {
    @State private var isRecording = false
    @State private var recognizedText: String = ""
    @State private var targetSentence: String = "Hello, world!"
    @State private var isCorrect: Bool = false
    @State private var recognitionRequest: SFSpeechAudioBufferRecognitionRequest?
    @State private var recognitionTask: SFSpeechRecognitionTask?
    let audioEngine = AVAudioEngine()
    let speechRecognizer = SFSpeechRecognizer()
    
    var body: some View {
        ZStack {
            Color(.systemGray6)
                .edgesIgnoringSafeArea(.all)
            
            VStack {
                Text("Target sentence: \(targetSentence)")
                    .font(.title)
                    .padding(.top)
                
                Text("Your sentence: \(recognizedText)")
                    .font(.title2)
                    .padding()
                
                Text(isCorrect ? "Correct!" : "Incorrect")
                    .font(.headline)
                    .foregroundColor(isCorrect ? .green : .red)
                    .padding(.bottom)
                
                Button(action: {
                    self.isRecording.toggle()
                    if self.isRecording {
                        do {
                            try self.startRecording()
                        } catch {
                            print("Failed to start recording: \(error)")
                        }
                    } else {
                        self.stopRecording()
                    }
                }) {
                    Text("\(isRecording ? "Stop" : "Start") recording")
                        .foregroundColor(.white)
                        .padding()
                        .background(isRecording ? Color.red : Color.blue)
                        .cornerRadius(10)
                }
                .padding(.bottom)
            }
            .overlay(RoundedRectangle(cornerRadius: 10).stroke(Color.gray, lineWidth: 1))
            .padding([.leading, .trailing], 10)
        }
    }
    
    func startRecording() throws {
        isCorrect = false
        recognizedText = ""
        
        let audioSession = AVAudioSession.sharedInstance()
        try audioSession.setCategory(.record, mode: .measurement, options: .duckOthers)
        try audioSession.setActive(true, options: .notifyOthersOnDeactivation)
        
        recognitionRequest = SFSpeechAudioBufferRecognitionRequest()
        guard let recognitionRequest = recognitionRequest else { return }
        
        let inputNode = audioEngine.inputNode
        recognitionRequest.shouldReportPartialResults = true
        
        recognitionTask = speechRecognizer?.recognitionTask(with: recognitionRequest, resultHandler: { (result, error) in
            if let result = result {
                self.recognizedText = result.bestTranscription.formattedString
                if self.recognizedText == self.targetSentence {
                    self.isCorrect = true
                }
            } else if let error = error {
                print("Recognition failed: \(error)")
            }
        })
        
        let recordingFormat = inputNode.outputFormat(forBus: 0)
        inputNode.installTap(onBus: 0, bufferSize: 1024, format: recordingFormat) { (buffer, _) in
            recognitionRequest.append(buffer)
        }
        
        audioEngine.prepare()
        try audioEngine.start()
    }
    
    func stopRecording() {
        audioEngine.stop()
        audioEngine.inputNode.removeTap(onBus: 0)
        
        recognitionRequest?.endAudio()
        recognitionTask?.cancel()
        
        recognitionRequest = nil
        recognitionTask = nil
    }
}


```

이 예제에서는 ZStack을 사용하여 배경색을 설정하고 VStack 내부에 있는 모든 요소를 overlay()를 사용하여 테두리가 있는 RoundedRectangle로 감쌌다. 
padding()을 이용하여 요소들 사이의 간격을 조정하고 버튼의 디자인도 수정하여 레코딩 상태에 따라 다른 색상을 보여주도록 했다.